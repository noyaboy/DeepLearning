{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e843c43b-417a-4647-a5bc-8261ef3c61a4",
   "metadata": {},
   "source": [
    "# Lab04 Task 2-2: Manual Post-Training Static Quantization\n",
    "\n",
    "In this notebook, we will try to manually quantize the pretrained model.\n",
    "\n",
    "<font color=\"red\">**Only add or modify code between `YOUR CODE START` and `YOUR CODE END`. Don’t change anything outside of these markers.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e004dd1-edb5-4f1c-88e9-49e3b4ad3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Please fill in your student id here.\n",
    "student_id = \"314580042\"\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe68ce-bb84-447a-be54-c7a44f4e5b39",
   "metadata": {},
   "source": [
    "### Library Import\n",
    "\n",
    "The libraries you need for this practice are listed below. You can add more if you think they’re necessary. If you’re not sure whether a library is allowed, ask TA in the FB group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b609c78-f0f5-4f25-aa7d-1ec8b3869ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import copy\n",
    "from resnet20_int8 import (\n",
    "    QuantizedTensor,\n",
    "    QuantizedCifarResNet,\n",
    "    QuantizeLayer,\n",
    "    QuantizedConv2d,\n",
    "    QuantizedConvReLU2d,\n",
    "    QuantizedReLU,\n",
    "    QuantizedLinear,\n",
    "    QuantizedAdaptiveAvgPool2d,\n",
    "    QuantizedAdd,\n",
    "    QuantizedFlatten,\n",
    ")\n",
    "import matplotlib\n",
    "\n",
    "##### YOUR CODE START #####\n",
    "\n",
    "# Do you need any additional libraries? If not, you can leave this block empty.\n",
    "# For this task, you must attempt to manually quantize the model. Therefore, using any libraries that perform automatic quantization or calculate scale/zero-point values is prohibited.\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f14f6-0076-47f8-b819-2b2b32b4f82a",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "If you have GPU available, you should see \"cuda\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a60e3d-c375-4e87-b2b0-ad3920295f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9520b-201d-4e40-bf6d-f539dbe2f5d6",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this lab, we will use CIFAR-10 dataset. CIFAR-10 is a widely used image classification dataset consisting of 60,000 color images at 32×32 resolution. It has 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck), with 50,000 training images and 10,000 test images. Due to its small size and balanced categories, CIFAR-10 is commonly used for benchmarking machine learning and computer vision models.\n",
    "\n",
    "CIFAR-10 has both a training set and a test set. Post-training static quantization requires a small subset of the training set for calibration. On the other hand, manually quantizing the convolutional layer weights is a data-free process. The test set is only used at the end to evaluate the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219993dc-4289-4509-970a-31e89686f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training & test set\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344f370-2f66-42f3-a8ed-d0383e4982fb",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "In this lab, you do not need to train a model from scratch. We will use a pretrained ResNet20 model instead. ResNet20 is a popular deep learning model for image classification. Its key feature is the use of skip (residual) connections, which make training deep networks easier and more stable.\n",
    "The code below loads the pre-trained model and evaluates its accuracy on the test set, which should be <font color=\"red\">**92.60%**</font>. Please use this model for the subsequent tasks. <font color=\"red\">**Designing and training your own model is not allowed.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a314efc-a14d-496c-b16d-a049b900783e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /export/home/dl2025f/dl2025f_116/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CifarResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('chenyaofo/pytorch-cifar-models', \n",
    "                       'cifar10_resnet20', pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857d22f9-4dfe-4c95-9555-070e4f3ba034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CIFAR-10 test set: 92.59%\n"
     ]
    }
   ],
   "source": [
    "def test_acc(model_test):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model_test(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print(f'Accuracy on CIFAR-10 test set: {acc:.2f}%')\n",
    "    return acc\n",
    "    \n",
    "_ = test_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d340f3-2552-4cb9-9c9e-20a86d25493e",
   "metadata": {},
   "source": [
    "### Manually Quantizing the Model\n",
    "\n",
    "While PyTorch's FX graph mode is very convenient, it abstracts away the details of how quantized weights are actually calculated. Therefore, in this section, you will attempt to manually quantize the model.\n",
    "\n",
    "To pass this lab, the test accuracy of your manually quantized model must be higher than <font color=\"red\">**90.00%.**</font>\n",
    "\n",
    "<font color=\"red\">**Please be aware of the following rules. Violating them will result in a score of zero for this section:**</font>\n",
    "\n",
    "1. Your modifications to the model are strictly limited to populating the parameters of the `QuantizedCifarResNet` model. Any other operations, including but not limited to retraining, or changing the model architecture, are forbidden.\n",
    "\n",
    "2. You must explicitly show your calculation process. The use of any functions that automatically compute scale / zero_point or gather statistics is prohibited. (The pre-defined observer in the previous task is prohibited, but it is allowed to use `torch.max` and `torch.min`, or define an observer on your own.) Also, you must not directly assign numerical values without demonstrating how they were derived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33203a-3244-4f15-a735-854fced5f833",
   "metadata": {},
   "source": [
    "### Introduction to QuantizedCifarResNet\n",
    "\n",
    "`QuantizedCifarResNet` is a modified version of the standard `CifarResNet` architecture, specifically adapted for **integer-only inference**. Unlike the original `CifarResNet` which performs computations using 32-bit floating-point (FP32) numbers, this quantized version primarily uses 8-bit integer arithmetic (`int8` for weights, `uint8` for activations) for most of its operations. This significantly reduces model size and can lead to faster inference speeds on hardware with specialized integer instruction support.\n",
    "\n",
    "The key differences arise from replacing standard PyTorch layers (`nn.Conv2d`, `nn.ReLU`, `nn.Linear`, etc.) with custom-defined quantized layer equivalents. These custom layers require specific **quantization parameters** (scale and zero-point) to map the integer values back to the approximate floating-point range, ensuring the model maintains reasonable accuracy. Data between these layers is passed using a `QuantizedTensor` wrapper object, which bundles the `uint8` tensor data with its corresponding `scale` and `zero_point`.\n",
    "\n",
    "Here's a breakdown of the custom quantized layers used in this implementation and the parameters they typically require **after initialization** (usually set via methods like `set_..._params` or `set_..._weight` after calibration):\n",
    "\n",
    "1.  **`QuantizeLayer`**:\n",
    "    * **Role**: The entry point, converts the initial `float32` input tensor into a `QuantizedTensor` (`uint8`).\n",
    "    * **Parameters Needed**:\n",
    "        * `output_scale (float)`: The scale factor for the output activation.\n",
    "        * `output_zero_point (int)`: The zero-point for the output activation.\n",
    "\n",
    "2.  **`QuantizedConv2d`** (Used for `conv2` in BasicBlock and `downsample`):\n",
    "    * **Role**: Performs 2D convolution using `int8` weights and `uint8` activations, producing a `uint8` output. Uses `fp32` bias.\n",
    "    * **Parameters Needed**:\n",
    "        * `weight_int8 (torch.Tensor[int8])`: The quantized weights (typically obtained after fusing BatchNorm).\n",
    "        * `weight_scale (torch.Tensor[float32])`: The **per-channel** scale factor for the weights.\n",
    "        * `weight_zero_point (torch.Tensor[int32])`: The **per-channel** zero-point for the weights.\n",
    "        * `bias_fp32 (torch.Tensor[float32])`: The fused `float32` bias term.\n",
    "        * `output_scale (float)`: The scale factor for the output activation.\n",
    "        * `output_zero_point (int)`: The zero-point for the output activation.\n",
    "\n",
    "3.  **`QuantizedConvReLU2d`** (Used for `conv1` in BasicBlock and the first `conv1` of the network):\n",
    "    * **Role**: Fuses `Conv2d` and `ReLU` operations. Similar to `QuantizedConv2d` but applies ReLU before the final requantization step. Uses `fp32` bias. **The output zero-point is implicitly 0 due to ReLU.**\n",
    "    * **Parameters Needed**:\n",
    "        * `weight_int8 (torch.Tensor[int8])`\n",
    "        * `weight_scale (torch.Tensor[float32])` (**Per-channel**)\n",
    "        * `weight_zero_point (torch.Tensor[int32])` (**Per-channel**)\n",
    "        * `bias_fp32 (torch.Tensor[float32])`\n",
    "        * `output_scale (float)` (Output zero-point is fixed to 0 internally).\n",
    "\n",
    "4.  **`QuantizedReLU`**:\n",
    "    * **Role**: Applies ReLU activation directly on the `uint8` tensor by clamping values below the input `zero_point`.\n",
    "    * **Parameters Needed**: None (It's stateless and uses the parameters from the input `QuantizedTensor`).\n",
    "\n",
    "5.  **`QuantizedAdd`**:\n",
    "    * **Role**: Performs element-wise addition of two `QuantizedTensor` inputs (requiring dequantization, float addition, and requantization). Used for residual connections.\n",
    "    * **Parameters Needed**:\n",
    "        * `output_scale (float)`: The scale factor for the resulting summed activation.\n",
    "        * `output_zero_point (int)`: The zero-point for the resulting summed activation.\n",
    "\n",
    "6.  **`QuantizedAdaptiveAvgPool2d`**:\n",
    "    * **Role**: Performs adaptive average pooling on the `uint8` tensor.\n",
    "    * **Parameters Needed**: None (Stateless, passes through the input scale/zero-point after integer averaging).\n",
    "\n",
    "7.  **`QuantizedFlatten`**:\n",
    "    * **Role**: Flattens the `uint8` tensor while preserving scale/zero-point.\n",
    "    * **Parameters Needed**: None (Stateless).\n",
    "\n",
    "8.  **`QuantizedLinear`** (Used as the final `fc` layer):\n",
    "    * **Role**: Performs a linear transformation using `int8` weights and `uint8` input, producing a `float32` output (common for the final classification layer). Uses `fp32` bias.\n",
    "    * **Parameters Needed**:\n",
    "        * `weight_int8 (torch.Tensor[int8])`\n",
    "        * `weight_scale (torch.Tensor[float32])` (**Per-channel/output feature**)\n",
    "        * `weight_zero_point (torch.Tensor[int32])` (**Per-channel/output feature**)\n",
    "        * `bias_fp32 (torch.Tensor[float32])`\n",
    "\n",
    "You can find more details of `QuantizedCifarResNet` in `resnet20_int8.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e186bf-9d88-47ad-a1b2-2859e94f145a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedCifarResNet(\n",
       "  (quant): QuantizeLayer(output_scale=1.000000, output_zero_point=0)\n",
       "  (conv1): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), padding=(0, 0), groups=1, bias=True)\n",
       "      )\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), padding=(0, 0), groups=1, bias=True)\n",
       "      )\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): QuantizedAdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flat): QuantizedFlatten()\n",
       "  (fc): QuantizedLinear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manual = QuantizedCifarResNet().to(device)\n",
    "model_manual.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b60bf-2a8d-4b00-b2f3-04da838feefb",
   "metadata": {},
   "source": [
    "#### 1. Prepare the Model\n",
    "\n",
    "Let's manually quantize the model step-by-step. We'll start with the preparation phase. Although PyTorch's FX graph mode offers `prepare_fx` to automatically insert observers and fuse layers (e.g., `Conv2d`, `BatchNorm2d`, `ReLU`), we need to do this manually here. So, we'll define our own observer now and insert it into the FP32 model. Layer fusion mainly concerns weight recalculation, so we'll handle that later in step three. Below is an example of defining a min/max observer and inserting it into the initial `bn1` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb47e3af-b2ab-45c8-a399-095709a0c3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered 32 observers on the model\n"
     ]
    }
   ],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Define the Observer class (can be used as a hook)\n",
    "class Observer:\n",
    "    def __init__(self):\n",
    "        # Initialize min/max to capture the first batch's range.\n",
    "        self.min_val = float('inf')\n",
    "        self.max_val = float('-inf')\n",
    "\n",
    "    def __call__(self, module: nn.Module, inputs: tuple, output: torch.Tensor):\n",
    "        # Hook function executed after the module's forward pass.\n",
    "        # Detach tensor for efficiency and get scalar min/max.\n",
    "        # module (nn.Module): The module the hook is registered on.\n",
    "        # inputs (tuple): A tuple containing the input(s) to the module.\n",
    "        # output (torch.Tensor): The output tensor from the module.\n",
    "        batch_min = output.detach().min().item()\n",
    "        batch_max = output.detach().max().item()\n",
    "\n",
    "        # Update overall min/max seen so far.\n",
    "        self.min_val = min(self.min_val, batch_min)\n",
    "        self.max_val = max(self.max_val, batch_max)\n",
    "\n",
    "    def get_min_max(self) -> tuple[float, float]:\n",
    "        # Returns the overall observed min and max values.\n",
    "        return self.min_val, self.max_val\n",
    "\n",
    "    def reset(self):\n",
    "        # Resets the observed min/max values.\n",
    "        self.min_val = float('inf')\n",
    "        self.max_val = float('-inf')\n",
    "\n",
    "# Create a copy to attach hooks without modifying the original model.\n",
    "model_prepared = copy.deepcopy(model)\n",
    "\n",
    "# Dictionary to store all observers for each layer we want to monitor\n",
    "observers = {}\n",
    "\n",
    "# We need to observe activations after each layer that will be quantized\n",
    "# For ResNet20, we need to observe:\n",
    "# 1. Input (after first conv+bn)\n",
    "# 2. After each Conv+BN or Conv+BN+ReLU fusion point\n",
    "# 3. After Add operations (residual connections)\n",
    "\n",
    "# Register observers on key layers\n",
    "# Initial layers\n",
    "observers['input'] = Observer()\n",
    "# Observe bn1 (pre-ReLU) so we can derive post-ReLU range for the first fused conv\n",
    "observers['bn1'] = Observer()\n",
    "model_prepared.bn1.register_forward_hook(observers['bn1'])\n",
    "# (optional) keep relu observer if you also use it elsewhere\n",
    "observers['relu'] = Observer()\n",
    "model_prepared.relu.register_forward_hook(observers['relu'])\n",
    "\n",
    "# Layer 1 (3 blocks)\n",
    "for i in range(3):\n",
    "    observers[f'layer1.{i}.bn1'] = Observer()\n",
    "    observers[f'layer1.{i}.bn2'] = Observer()\n",
    "    observers[f'layer1.{i}.relu'] = Observer()\n",
    "    model_prepared.layer1[i].bn1.register_forward_hook(observers[f'layer1.{i}.bn1'])\n",
    "    model_prepared.layer1[i].bn2.register_forward_hook(observers[f'layer1.{i}.bn2'])\n",
    "    model_prepared.layer1[i].relu.register_forward_hook(observers[f'layer1.{i}.relu'])\n",
    "\n",
    "# Layer 2 (3 blocks)\n",
    "for i in range(3):\n",
    "    observers[f'layer2.{i}.bn1'] = Observer()\n",
    "    observers[f'layer2.{i}.bn2'] = Observer()\n",
    "    observers[f'layer2.{i}.relu'] = Observer()\n",
    "    model_prepared.layer2[i].bn1.register_forward_hook(observers[f'layer2.{i}.bn1'])\n",
    "    model_prepared.layer2[i].bn2.register_forward_hook(observers[f'layer2.{i}.bn2'])\n",
    "    model_prepared.layer2[i].relu.register_forward_hook(observers[f'layer2.{i}.relu'])\n",
    "    # Downsample in first block\n",
    "    if i == 0 and model_prepared.layer2[i].downsample is not None:\n",
    "        observers['layer2.0.downsample.1'] = Observer()\n",
    "        model_prepared.layer2[i].downsample[1].register_forward_hook(observers['layer2.0.downsample.1'])\n",
    "\n",
    "# Layer 3 (3 blocks)\n",
    "for i in range(3):\n",
    "    observers[f'layer3.{i}.bn1'] = Observer()\n",
    "    observers[f'layer3.{i}.bn2'] = Observer()\n",
    "    observers[f'layer3.{i}.relu'] = Observer()\n",
    "    model_prepared.layer3[i].bn1.register_forward_hook(observers[f'layer3.{i}.bn1'])\n",
    "    model_prepared.layer3[i].bn2.register_forward_hook(observers[f'layer3.{i}.bn2'])\n",
    "    model_prepared.layer3[i].relu.register_forward_hook(observers[f'layer3.{i}.relu'])\n",
    "    # Downsample in first block\n",
    "    if i == 0 and model_prepared.layer3[i].downsample is not None:\n",
    "        observers['layer3.0.downsample.1'] = Observer()\n",
    "        model_prepared.layer3[i].downsample[1].register_forward_hook(observers['layer3.0.downsample.1'])\n",
    "\n",
    "print(f\"Registered {len(observers)} observers on the model\")\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26372dd9-4373-47e8-aa24-839f3fa52e74",
   "metadata": {},
   "source": [
    "#### 2. Calibrate the Model\n",
    "\n",
    "Next, we need to calibrate the model. This step is quite similar to the process when using PyTorch's FX graph mode. It simply involves feeding data from the training set (or a representative subset of it) through the model we prepared earlier (the one with observers attached). Please note: <font color=\"red\">**it is crucial not to use the test set data for calibration.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854bd203-9a73-4cf8-9c6e-90425c9c5923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calibration...\n",
      "Calibration completed with 50 batches\n",
      "Input range: [-2.4291, 2.7537]\n",
      "\n",
      "Sample observer statistics:\n",
      "relu (first layer): [0.0000, 3.8333]\n",
      "layer1.0.relu: [0.0000, 4.6231]\n"
     ]
    }
   ],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Perform calibration here.\n",
    "\n",
    "# Use a subset of training data for calibration\n",
    "calibration_batches = 50  # Use 50 batches (6400 samples) for calibration\n",
    "\n",
    "print(\"Starting calibration...\")\n",
    "model_prepared.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_count = 0\n",
    "    # Track input statistics separately\n",
    "    input_min = float('inf')\n",
    "    input_max = float('-inf')\n",
    "    \n",
    "    for images, _ in trainloader:\n",
    "        if batch_count >= calibration_batches:\n",
    "            break\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Track input statistics\n",
    "        batch_input_min = images.min().item()\n",
    "        batch_input_max = images.max().item()\n",
    "        input_min = min(input_min, batch_input_min)\n",
    "        input_max = max(input_max, batch_input_max)\n",
    "        \n",
    "        # Forward pass to collect statistics via observers\n",
    "        _ = model_prepared(images)\n",
    "        batch_count += 1\n",
    "    \n",
    "    observers['input'].min_val = input_min\n",
    "    observers['input'].max_val = input_max\n",
    "\n",
    "print(f\"Calibration completed with {batch_count} batches\")\n",
    "print(f\"Input range: [{input_min:.4f}, {input_max:.4f}]\")\n",
    "\n",
    "# Print some statistics to verify calibration\n",
    "print(\"\\nSample observer statistics:\")\n",
    "print(f\"relu (first layer): [{observers['relu'].min_val:.4f}, {observers['relu'].max_val:.4f}]\")\n",
    "print(f\"layer1.0.relu: [{observers['layer1.0.relu'].min_val:.4f}, {observers['layer1.0.relu'].max_val:.4f}]\")\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f59cc5-0173-48a0-a9c8-7e07b02cc6ac",
   "metadata": {},
   "source": [
    "#### 3. Convert the Model\n",
    "\n",
    "Finally, we need to populate the `QuantizedCifarResNet` with its parameters. You will need to iterate through all layers in the quantized model and set the required parameters (such as quantized weights, scales, zero-points, and biases) based on their type. The necessary data should be obtained from the original model and the observers inserted previously. Additionally, note that consecutive `Conv2d`, `BatchNorm2d`, and `ReLU` layers in the original model have been fused into corresponding single layers in `QuantizedCifarResNet`. You must adjust the `Conv2d` weights and biases according to the parameters of the corresponding `BatchNorm2d` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3e1a26-3573-4201-b85d-ed43a257d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual quantization...\n",
      "\n",
      "1. Quantizing input layer...\n",
      "   Input: scale=0.020325, zp=120\n",
      "\n",
      "2. Quantizing conv1 + bn1 + relu (fused)...\n",
      "   Conv1: weight_scale shape=torch.Size([16]), output_scale=0.015032\n",
      "\n",
      "3. Quantizing layer1...\n",
      "   Block 0:\n",
      "      conv1: output_scale=0.009787\n",
      "      conv2: output_scale=0.025925, output_zp=117\n",
      "      add: output_scale=0.018130, output_zp=0\n",
      "   Block 1:\n",
      "      conv1: output_scale=0.010260\n",
      "      conv2: output_scale=0.018620, output_zp=94\n",
      "      add: output_scale=0.022498, output_zp=0\n",
      "   Block 2:\n",
      "      conv1: output_scale=0.008037\n",
      "      conv2: output_scale=0.026892, output_zp=126\n",
      "      add: output_scale=0.019750, output_zp=0\n",
      "\n",
      "4. Quantizing layer2...\n",
      "   Block 0:\n",
      "      conv1: output_scale=0.011693\n",
      "      conv2: output_scale=0.024414, output_zp=130\n",
      "      Processing downsample...\n",
      "         downsample: output_scale=0.016035, output_zp=103\n",
      "      add: output_scale=0.018474, output_zp=0\n",
      "   Block 1:\n",
      "      conv1: output_scale=0.006851\n",
      "      conv2: output_scale=0.018758, output_zp=110\n",
      "      add: output_scale=0.020946, output_zp=0\n",
      "   Block 2:\n",
      "      conv1: output_scale=0.009456\n",
      "      conv2: output_scale=0.034665, output_zp=81\n",
      "      add: output_scale=0.032946, output_zp=0\n",
      "\n",
      "5. Quantizing layer3...\n",
      "   Block 0:\n",
      "      conv1: output_scale=0.010605\n",
      "      conv2: output_scale=0.020407, output_zp=104\n",
      "      Processing downsample...\n",
      "         downsample: output_scale=0.008543, output_zp=110\n",
      "      add: output_scale=0.015487, output_zp=0\n",
      "   Block 1:\n",
      "      conv1: output_scale=0.009784\n",
      "      conv2: output_scale=0.031795, output_zp=142\n",
      "      add: output_scale=0.021968, output_zp=0\n",
      "   Block 2:\n",
      "      conv1: output_scale=0.008913\n",
      "      conv2: output_scale=0.095906, output_zp=81\n",
      "      add: output_scale=0.069958, output_zp=0\n",
      "\n",
      "6. Quantizing FC layer...\n",
      "   FC: weight_scale shape=torch.Size([10])\n",
      "\n",
      "Manual quantization completed!\n"
     ]
    }
   ],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Iterate through all layers in the quantized model and set the required parameters (such as quantized weights, scales, zero-points, and biases) based on their type.\n",
    "\n",
    "# Helper functions for quantization calculations\n",
    "def calculate_scale_zero_point(min_val, max_val, dtype='uint8'):\n",
    "    \"\"\"\n",
    "    Calculate scale and zero_point for quantization using min-max method.\n",
    "    \n",
    "    For uint8: range is [0, 255]\n",
    "    Formula:\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - round(min_val / scale)\n",
    "    \n",
    "    where qmin=0, qmax=255 for uint8\n",
    "    \"\"\"\n",
    "    if dtype == 'uint8':\n",
    "        qmin, qmax = 0, 255\n",
    "    elif dtype == 'int8':\n",
    "        qmin, qmax = -128, 127\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if max_val == min_val:\n",
    "        scale = 1.0\n",
    "        zero_point = 0\n",
    "    else:\n",
    "        # Calculate scale\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        # Calculate zero_point\n",
    "        zero_point = qmin - round(min_val / scale)\n",
    "        # Clamp zero_point to valid range\n",
    "        zero_point = max(qmin, min(qmax, zero_point))\n",
    "    \n",
    "    return scale, int(zero_point)\n",
    "\n",
    "def quantize_weight_per_channel(weight, dtype='int8'):\n",
    "    \"\"\"\n",
    "    Quantize weights using per-channel (per output channel) quantization.\n",
    "    Returns: weight_int8, weight_scale, weight_zero_point\n",
    "    \n",
    "    For each output channel:\n",
    "        - Find min/max of weights in that channel\n",
    "        - Calculate scale and zero_point\n",
    "        - Quantize: q = round(w / scale) + zero_point\n",
    "    \"\"\"\n",
    "    out_channels = weight.shape[0]\n",
    "    weight_int = torch.zeros_like(weight, dtype=torch.int8)\n",
    "    weight_scale = torch.zeros(out_channels, dtype=torch.float32)\n",
    "    weight_zero_point = torch.zeros(out_channels, dtype=torch.int32)\n",
    "    \n",
    "    for i in range(out_channels):\n",
    "        channel_weight = weight[i]\n",
    "        # Calculate min/max for this channel\n",
    "        w_min = torch.min(channel_weight).item()\n",
    "        w_max = torch.max(channel_weight).item()\n",
    "        \n",
    "        # Calculate scale and zero_point\n",
    "        scale, zp = calculate_scale_zero_point(w_min, w_max, dtype='int8')\n",
    "        \n",
    "        # Quantize the weights\n",
    "        w_quant = torch.round(channel_weight / scale) + zp\n",
    "        w_quant = torch.clamp(w_quant, -128, 127)\n",
    "        \n",
    "        weight_int[i] = w_quant.to(torch.int8)\n",
    "        weight_scale[i] = scale\n",
    "        weight_zero_point[i] = zp\n",
    "    \n",
    "    return weight_int, weight_scale, weight_zero_point\n",
    "\n",
    "def fuse_conv_bn(conv, bn):\n",
    "    \"\"\"\n",
    "    Fuse Conv2d and BatchNorm2d layers.\n",
    "    \n",
    "    BatchNorm formula: y = gamma * (x - mean) / sqrt(var + eps) + beta\n",
    "    After fusion with conv:\n",
    "        w_fused = w_conv * gamma / sqrt(var + eps)\n",
    "        b_fused = (b_conv - mean) * gamma / sqrt(var + eps) + beta\n",
    "    \n",
    "    If conv has no bias: b_conv = 0\n",
    "    \"\"\"\n",
    "    # Get conv parameters\n",
    "    w_conv = conv.weight.data\n",
    "    if conv.bias is not None:\n",
    "        b_conv = conv.bias.data\n",
    "    else:\n",
    "        b_conv = torch.zeros(conv.out_channels, device=w_conv.device, dtype=w_conv.dtype)\n",
    "    \n",
    "    # Get BN parameters\n",
    "    gamma = bn.weight.data\n",
    "    beta = bn.bias.data\n",
    "    mean = bn.running_mean.data\n",
    "    var = bn.running_var.data\n",
    "    eps = bn.eps\n",
    "    \n",
    "    # Calculate fusion parameters\n",
    "    # std = sqrt(var + eps)\n",
    "    std = torch.sqrt(var + eps)\n",
    "    \n",
    "    # w_fused = w_conv * gamma / std\n",
    "    # Broadcasting: gamma/std has shape [out_channels], w_conv has shape [out_channels, in_channels, k, k]\n",
    "    scale_factor = (gamma / std).view(-1, 1, 1, 1)\n",
    "    w_fused = w_conv * scale_factor\n",
    "    \n",
    "    # b_fused = (b_conv - mean) * gamma / std + beta\n",
    "    b_fused = (b_conv - mean) * gamma / std + beta\n",
    "    \n",
    "    return w_fused, b_fused\n",
    "\n",
    "# Start populating the quantized model\n",
    "print(\"Starting manual quantization...\")\n",
    "\n",
    "# Get the original model's modules for reference\n",
    "orig_modules = dict(model.named_modules())\n",
    "\n",
    "# 1. QuantizeLayer (input quantization)\n",
    "print(\"\\n1. Quantizing input layer...\")\n",
    "input_scale, input_zp = calculate_scale_zero_point(\n",
    "    observers['input'].min_val,\n",
    "    observers['input'].max_val,\n",
    "    dtype='uint8'\n",
    ")\n",
    "model_manual.quant.set_output_quant_params(input_scale, input_zp)\n",
    "print(f\"   Input: scale={input_scale:.6f}, zp={input_zp}\")\n",
    "\n",
    "# 2. First conv1 + bn1 + relu (fused into QuantizedConvReLU2d)\n",
    "# Use bn1 observer (pre-ReLU) to derive post-ReLU range, avoiding mixing with block-end ReLU\n",
    "print(\"\\n2. Quantizing conv1 + bn1 + relu (fused)...\")\n",
    "conv1 = orig_modules['conv1']\n",
    "bn1 = orig_modules['bn1']\n",
    "w_fused, b_fused = fuse_conv_bn(conv1, bn1)\n",
    "w_int8, w_scale, w_zp = quantize_weight_per_channel(w_fused)\n",
    "relu_min, relu_max = 0.0, max(0.0, observers['bn1'].max_val)\n",
    "output_scale, _ = calculate_scale_zero_point(relu_min, relu_max, dtype='uint8')\n",
    "model_manual.conv1.set_int8_weight(w_int8)\n",
    "model_manual.conv1.set_weight_quant_params(w_scale, w_zp)\n",
    "model_manual.conv1.set_fp32_bias(b_fused)\n",
    "model_manual.conv1.set_output_quant_params(output_scale)\n",
    "print(f\"   Conv1: weight_scale shape={w_scale.shape}, output_scale={output_scale:.6f}\")\n",
    "\n",
    "# 3. Process each layer (layer1, layer2, layer3)\n",
    "for layer_idx, layer_name in enumerate(['layer1', 'layer2', 'layer3']):\n",
    "    print(f\"\\n{3+layer_idx}. Quantizing {layer_name}...\")\n",
    "    layer = getattr(model_manual, layer_name)\n",
    "    orig_layer = getattr(model, layer_name)\n",
    "    \n",
    "    for block_idx in range(3):\n",
    "        print(f\"   Block {block_idx}:\")\n",
    "        block = layer[block_idx]\n",
    "        orig_block = orig_layer[block_idx]\n",
    "        \n",
    "        # Conv1 + BN1 (fused into QuantizedConvReLU2d)\n",
    "        w_fused, b_fused = fuse_conv_bn(orig_block.conv1, orig_block.bn1)\n",
    "        w_int8, w_scale, w_zp = quantize_weight_per_channel(w_fused)\n",
    "        # derive post-ReLU range from bn1 stats: [0, max(0, bn1_max)]\n",
    "        relu_min, relu_max = 0.0, max(0.0, observers[f'{layer_name}.{block_idx}.bn1'].max_val)\n",
    "        output_scale, _ = calculate_scale_zero_point(relu_min, relu_max, dtype='uint8')\n",
    "        block.conv1.set_int8_weight(w_int8)\n",
    "        block.conv1.set_weight_quant_params(w_scale, w_zp)\n",
    "        block.conv1.set_fp32_bias(b_fused)\n",
    "        block.conv1.set_output_quant_params(output_scale)\n",
    "        print(f\"      conv1: output_scale={output_scale:.6f}\")\n",
    "        \n",
    "        # Conv2 + BN2 (fused into QuantizedConv2d)\n",
    "        w_fused, b_fused = fuse_conv_bn(orig_block.conv2, orig_block.bn2)\n",
    "        w_int8, w_scale, w_zp = quantize_weight_per_channel(w_fused)\n",
    "        output_scale, output_zp = calculate_scale_zero_point(\n",
    "            observers[f'{layer_name}.{block_idx}.bn2'].min_val,\n",
    "            observers[f'{layer_name}.{block_idx}.bn2'].max_val,\n",
    "            dtype='uint8'\n",
    "        )\n",
    "        block.conv2.set_int8_weight(w_int8)\n",
    "        block.conv2.set_weight_quant_params(w_scale, w_zp)\n",
    "        block.conv2.set_fp32_bias(b_fused)\n",
    "        block.conv2.set_output_quant_params(output_scale, output_zp)\n",
    "        print(f\"      conv2: output_scale={output_scale:.6f}, output_zp={output_zp}\")\n",
    "        \n",
    "        # Downsample (if exists)\n",
    "        if orig_block.downsample is not None:\n",
    "            print(f\"      Processing downsample...\")\n",
    "            downsample_conv = orig_block.downsample[0]\n",
    "            downsample_bn = orig_block.downsample[1]\n",
    "            w_fused, b_fused = fuse_conv_bn(downsample_conv, downsample_bn)\n",
    "            w_int8, w_scale, w_zp = quantize_weight_per_channel(w_fused)\n",
    "            output_scale, output_zp = calculate_scale_zero_point(\n",
    "                observers[f'{layer_name}.{block_idx}.downsample.1'].min_val,\n",
    "                observers[f'{layer_name}.{block_idx}.downsample.1'].max_val,\n",
    "                dtype='uint8'\n",
    "            )\n",
    "            block.downsample[0].set_int8_weight(w_int8)\n",
    "            block.downsample[0].set_weight_quant_params(w_scale, w_zp)\n",
    "            block.downsample[0].set_fp32_bias(b_fused)\n",
    "            block.downsample[0].set_output_quant_params(output_scale, output_zp)\n",
    "            print(f\"         downsample: output_scale={output_scale:.6f}, output_zp={output_zp}\")\n",
    "        \n",
    "        # QuantizedAdd (after residual connection)\n",
    "        add_output_scale, add_output_zp = calculate_scale_zero_point(\n",
    "            observers[f'{layer_name}.{block_idx}.relu'].min_val,\n",
    "            observers[f'{layer_name}.{block_idx}.relu'].max_val,\n",
    "            dtype='uint8'\n",
    "        )\n",
    "        block.add.set_output_quant_params(add_output_scale, add_output_zp)\n",
    "        print(f\"      add: output_scale={add_output_scale:.6f}, output_zp={add_output_zp}\")\n",
    "\n",
    "# 4. Final FC layer\n",
    "print(\"\\n6. Quantizing FC layer...\")\n",
    "fc = orig_modules['fc']\n",
    "w = fc.weight.data\n",
    "b = fc.bias.data if fc.bias is not None else torch.zeros(fc.out_features, device=w.device, dtype=w.dtype)\n",
    "w_int8, w_scale, w_zp = quantize_weight_per_channel(w)\n",
    "model_manual.fc.set_int8_weight(w_int8)\n",
    "model_manual.fc.set_weight_quant_params(w_scale, w_zp)\n",
    "model_manual.fc.set_fp32_bias(b)\n",
    "print(f\"   FC: weight_scale shape={w_scale.shape}\")\n",
    "\n",
    "print(\"\\nManual quantization completed!\")\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de2e186-d070-48c3-ad01-2cf82a3c37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CIFAR-10 test set: 92.49%\n",
      "\n",
      "===========================================\n",
      "\n",
      "Congratulations! You've achieved the goal of this task. Remember to save your model!\n",
      "You can also try increasing accuracy further to earn a higher score!\n"
     ]
    }
   ],
   "source": [
    "# Let's see the result.\n",
    "acc = test_acc(model_manual)\n",
    "\n",
    "print(\"\\n===========================================\\n\")\n",
    "\n",
    "if acc < 90.0:\n",
    "    print(\"Oh no! Your test accuracy is too low!\")\n",
    "else:\n",
    "    print(\"Congratulations! You've achieved the goal of this task. Remember to save your model!\")\n",
    "    print(\"You can also try increasing accuracy further to earn a higher score!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46829a08-661e-40fc-a291-e367b4497797",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "You can use the code below to save your model as `[student_id]_quantization.pt`, where `[student_id]` is replaced by your student ID in the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e79133-d139-40fa-a3a1-12ec6660a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is saved to \"314580042_quantization.pt\".\n"
     ]
    }
   ],
   "source": [
    "file_name = student_id + \"_quantization.pt\"\n",
    "# Save model.state_dict() instead of the entire model.\n",
    "torch.save(model_manual.state_dict(), file_name)\n",
    "print(\"Your model is saved to \\\"\" + file_name + \"\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6845b-7899-4fe2-b330-15c2d36aff93",
   "metadata": {},
   "source": [
    "### Final Check\n",
    "\n",
    "TA has provided check_quantization.py for students to check if their models can pass the tests. <font color=\"red\">**Please make sure to check it before submission.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9f03e7-f966-403f-87c4-f83ecfac90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! You've achieved the goals of this task.\n",
      "Your model's test accuracy is 92.49%.\n"
     ]
    }
   ],
   "source": [
    "!python check_quantization.py --path {file_name}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
